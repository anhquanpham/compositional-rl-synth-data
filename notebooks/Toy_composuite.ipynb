{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb3e157d-acd0-4146-b452-31c4688a4173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Optional\n",
    "\n",
    "import gin\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "from einops import rearrange\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5db491f9-2197-4db0-b8c5-c8d4b09cefdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class compositional_mlp(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sizes,\n",
    "        activation,\n",
    "        num_modules, \n",
    "        module_assignment_positions, \n",
    "        module_inputs, \n",
    "        interface_depths,\n",
    "        graph_structure,\n",
    "        output_activation=nn.Identity\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._num_modules = num_modules\n",
    "        self.module_assignment_positions = module_assignment_positions\n",
    "        self._module_inputs = module_inputs         # keys in a dict\n",
    "        self._interface_depths = interface_depths\n",
    "        self._graph_structure = graph_structure     # [[0], [1,2], 3] or [[0], [1], [2], [3]]   \n",
    "\n",
    "        self._module_list = nn.ModuleList() # e.g., object, robot, task...\n",
    "        \n",
    "        for graph_depth in range(len(graph_structure)): # root -> children -> ... leaves \n",
    "            for j in graph_structure[graph_depth]:          # loop over all module types at this depth\n",
    "                self._module_list.append(nn.ModuleDict())   # pre, post\n",
    "                self._module_list[j]['pre_interface'] = nn.ModuleList()\n",
    "                self._module_list[j]['post_interface'] = nn.ModuleList()\n",
    "                \n",
    "                for k in range(num_modules[j]):                 # loop over all modules of this type\n",
    "                    layers_pre = []\n",
    "                    layers_post = []\n",
    "                    for i in range(len(sizes[j]) - 1):              # loop over all depths in this module\n",
    "                        act = activation if graph_depth < len(graph_structure) - 1 or i < len(sizes[j])-2 else output_activation\n",
    "\n",
    "                        if i == interface_depths[j]:\n",
    "                            input_size = sum(sizes[j_prev][-1] for j_prev in graph_structure[graph_depth - 1])\n",
    "                            input_size += sizes[j][i]\n",
    "                        else:\n",
    "                            input_size = sizes[j][i]\n",
    "\n",
    "                        new_layer = [nn.Linear(input_size, sizes[j][i+1]), act()]\n",
    "                        if i < interface_depths[j]:\n",
    "                            layers_pre += new_layer\n",
    "                        else:\n",
    "                            layers_post += new_layer\n",
    "                    if layers_pre:\n",
    "                        self._module_list[j]['pre_interface'].append(nn.Sequential(*layers_pre))\n",
    "                    else:   # it's either a root or a module with no preprocessing\n",
    "                        self._module_list[j]['pre_interface'].append(nn.Identity())\n",
    "                    self._module_list[j]['post_interface'].append(nn.Sequential(*layers_post))\n",
    "\n",
    "    def forward(self, input_val):\n",
    "        x = None\n",
    "        for graph_depth in range(len(self._graph_structure)):     # root -> children -> ... -> leaves\n",
    "            x_post = []\n",
    "            for j in self._graph_structure[graph_depth]:          # nodes (modules) at this depth\n",
    "                if len(input_val.shape) == 1:\n",
    "                    x_pre = input_val[self._module_inputs[j]]\n",
    "                    onehot = input_val[self.module_assignment_positions[j]]\n",
    "                else:\n",
    "                    x_pre = input_val[:, self._module_inputs[j]]\n",
    "                    onehot = input_val[0, self.module_assignment_positions[j]]\n",
    "                    assert (input_val[:, self.module_assignment_positions[j]] == onehot).all()\n",
    "                module_index = onehot.argmax()\n",
    "\n",
    "                print(self._module_list[j]['pre_interface'][module_index])\n",
    "                print(x_pre)\n",
    "                x_pre = self._module_list[j]['pre_interface'][module_index](x_pre)\n",
    "                if x is not None: x_pre = torch.cat((x, x_pre), dim=-1)\n",
    "                x_post.append(self._module_list[j]['post_interface'][module_index](x_pre))\n",
    "            x = torch.cat(x_post, dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bca16b51-3670-442a-933e-6a4697e4b832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([ 1.2011,  0.1963,  0.7617,  0.2197,  0.7114, -1.1768, -0.9514, -1.6777,\n",
      "        -0.2301,  0.1120, -0.0690,  0.1586,  0.6890,  0.0356]), tensor([ 1.4509,  0.8889,  0.1023, -0.4460, -0.2469, -0.0178,  0.5129,  2.0603,\n",
      "         0.4722, -1.2127, -0.8660,  0.6401,  0.0864,  0.1021]), tensor([ 2.9942, -0.2817, -0.4680,  0.2616,  0.1397,  1.2501,  0.1648, -0.1226,\n",
      "        -1.4296, -0.3062, -1.3422,  0.1108,  0.1556,  1.1018,  0.0235, -0.1095,\n",
      "         2.7542]), tensor([-1.8966, -0.6748,  0.0033,  1.2975,  0.1815,  0.9043,  0.7674,  1.6319,\n",
      "         1.7122, -0.9976, -0.3286,  0.9625, -1.4661, -0.4840,  2.1565,  0.8144,\n",
      "         0.4366,  0.4033,  2.0544, -1.4394, -0.1928, -0.4410, -0.5871,  1.0863,\n",
      "         0.4419, -1.9019, -0.5759,  1.0635, -0.1198, -1.8619,  1.2086,  0.1176])]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 60\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(input_val)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Forward pass through the network\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Print output to investigate\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput of the network:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output)\n",
      "File \u001b[0;32m~/.pyenv/versions/first_3.9.6/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/first_3.9.6/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[24], line 56\u001b[0m, in \u001b[0;36mcompositional_mlp.forward\u001b[0;34m(self, input_val)\u001b[0m\n\u001b[1;32m     54\u001b[0m x_post \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_structure[graph_depth]:          \u001b[38;5;66;03m# nodes (modules) at this depth\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43minput_val\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     57\u001b[0m         x_pre \u001b[38;5;241m=\u001b[39m input_val[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_module_inputs[j]]\n\u001b[1;32m     58\u001b[0m         onehot \u001b[38;5;241m=\u001b[39m input_val[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_assignment_positions[j]]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# Define the input positions and sizes based on your input structure\n",
    "module_inputs = {\n",
    "    0: [0],  # 'object-state' (14)\n",
    "    1: [14],  # 'obstacle-state' (14)\n",
    "    2: [28],  # 'goal-state' (17)\n",
    "    3: [45],  # 'robot0_proprio-state' (32)\n",
    "}\n",
    "\n",
    "# Define module sizes based on the input sizes and positions\n",
    "sizes = [\n",
    "    [14, 32, 64],  # 'object-state' (input size: 14)\n",
    "    [14, 32, 64],  # 'obstacle-state' (input size: 14)\n",
    "    [17, 32, 64],  # 'goal-state' (input size: 17)\n",
    "    [32, 64, 128], # 'robot0_proprio-state' (input size: 32)\n",
    "]\n",
    "\n",
    "# Activation function and other parameters\n",
    "activation = nn.ReLU\n",
    "num_modules = [4, 4, 4, 4]  # Four module per state\n",
    "module_assignment_positions = [0, 1, 2, 3]  \n",
    "interface_depths = [1, 1, 2, 3]  \n",
    "graph_structure = [[0], [1], [2], [3]]  \n",
    "\n",
    "# Initialize the model\n",
    "model = compositional_mlp(\n",
    "    sizes=sizes,\n",
    "    activation=activation,\n",
    "    num_modules=num_modules,\n",
    "    module_assignment_positions=module_assignment_positions,\n",
    "    module_inputs=module_inputs,\n",
    "    interface_depths=interface_depths,\n",
    "    graph_structure=graph_structure,\n",
    ")\n",
    "\n",
    "# Create a sample input tensor based on the input structure\n",
    "#input_val = torch.randn(77)  # 14 + 14 + 17 + 32 = 77 (total size)\n",
    "\n",
    "# Sizes for each state type\n",
    "object_state_size = 14\n",
    "obstacle_state_size = 14\n",
    "goal_state_size = 17\n",
    "robot_state_size = 32\n",
    "\n",
    "# Create random input vectors for each state (nested array format)\n",
    "object_state = torch.randn(object_state_size)  # Size [14]\n",
    "obstacle_state = torch.randn(obstacle_state_size)  # Size [14]\n",
    "goal_state = torch.randn(goal_state_size)  # Size [17]\n",
    "robot_state = torch.randn(robot_state_size)  # Size [32]\n",
    "\n",
    "input_val = [\n",
    "    object_state,  # size: [14]\n",
    "    obstacle_state,  # size: [14]\n",
    "    goal_state,  # size: [17]\n",
    "    robot_state  # size: [32]\n",
    "]\n",
    "\n",
    "print(input_val)\n",
    "\n",
    "# Forward pass through the network\n",
    "output = model(input_val)\n",
    "\n",
    "# Print output to investigate\n",
    "print(\"Output of the network:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6cc5fe-ad0b-49db-8ce2-cc1c9fb6895d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f7368b-fa7b-4c74-9110-a06bba0948ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "First 3.9.6",
   "language": "python",
   "name": "first_3.9.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
