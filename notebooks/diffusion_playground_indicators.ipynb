{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import composuite\n",
    "from diffusion.utils import *\n",
    "from diffusion.elucidated_diffusion import Trainer\n",
    "from diffusion.train_diffuser import SimpleDiffusionGenerator\n",
    "\n",
    "gin.parse_config_file(\"/home/anhquanpham/projects/compositional-rl-synth-data/config/diffusion.gin\")\n",
    "\n",
    "base_data_path = '/home/anhquanpham/projects/data'\n",
    "base_results_folder = '/home/anhquanpham/projects/results'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_indicator_vectors(modality_dims, dataset):\n",
    "\n",
    "    dataset = dataset.copy()\n",
    "\n",
    "    start_index = sum([dim[0] for key, dim in modality_dims.items() if key in ['object-state', 'obstacle-state', 'goal-state']])\n",
    "    end_index = start_index + sum([dim[0] for key, dim in modality_dims.items() if key in ['object_id', 'robot_id', 'obstacle_id', 'subtask_id']])\n",
    "\n",
    "    def remove_indicator_dims(data, start, end):\n",
    "        indicators_ = data[:, start:end]\n",
    "        data_ = np.delete(data, np.arange(start, end), axis=1)\n",
    "        return data_, indicators_\n",
    "\n",
    "    observations, obs_indicators = remove_indicator_dims(dataset['observations'], start_index, end_index)\n",
    "    dataset['observations'] = observations\n",
    "    next_observations, _ = remove_indicator_dims(dataset['next_observations'], start_index, end_index)\n",
    "    dataset['next_observations'] = next_observations\n",
    "    \n",
    "    return dataset, obs_indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anhquanpham/.pyenv/versions/first_3.9.6/lib/python3.9/site-packages/gym/spaces/box.py:128: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing task indicators: (999999, 93)\n",
      "After removing task indicators: (999999, 77) (999999, 16)\n",
      "{'object-state': (14,), 'obstacle-state': (14,), 'goal-state': (17,), 'object_id': (4,), 'robot_id': (4,), 'obstacle_id': (4,), 'subtask_id': (4,), 'robot0_proprio-state': (32,)}\n",
      "dict_keys(['observations', 'actions', 'next_observations', 'rewards', 'terminals'])\n"
     ]
    }
   ],
   "source": [
    "dataset_type = 'expert'\n",
    "\n",
    "robot = 'IIWA'\n",
    "obj = 'Box'\n",
    "obst = 'None'\n",
    "task = 'PickPlace'\n",
    "\n",
    "results_folder = os.path.join(base_results_folder, robot + '_' + obj + '_' + obst + '_' + task)\n",
    "\n",
    "\n",
    "\n",
    "env = composuite.make(robot, obj, obst, task, use_task_id_obs=True, ignore_done=False)\n",
    "dataset = load_single_composuite_dataset(base_path=base_data_path, \n",
    "                                         dataset_type=dataset_type, \n",
    "                                         robot=robot, obj=obj, \n",
    "                                         obst=obst, task=task)\n",
    "dataset = transitions_dataset(dataset)\n",
    "\n",
    "print('Before removing task indicators:', dataset['observations'].shape)\n",
    "dataset, indicators = remove_indicator_vectors(env.modality_dims, dataset)\n",
    "print('After removing task indicators:', dataset['observations'].shape, indicators.shape)\n",
    "print(env.modality_dims)\n",
    "\n",
    "print(dataset.keys())  # Should still contain 'observations' and 'next_observations'\n",
    "\n",
    "\n",
    "inputs = make_inputs(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.61086540e-02 -1.57110706e-01  8.50000024e-01  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.00000000e+00  6.70293272e-02\n",
      "  5.71107194e-02  1.56766713e-01  9.99999702e-01  0.00000000e+00\n",
      "  7.96326727e-04  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  2.95669660e-02 -1.00000024e-01  1.00670826e+00\n",
      "  9.99999702e-01  0.00000000e+00  7.96326727e-04  0.00000000e+00\n",
      "  1.00000001e-01  1.80000007e-01  8.29999983e-01  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.00000000e+00  1.30888745e-01\n",
      " -2.80000061e-01  1.76868483e-01  9.99999702e-01  0.00000000e+00\n",
      "  7.96326727e-04  0.00000000e+00  6.38913438e-02  3.37110698e-01\n",
      " -1.99999996e-02  1.00000000e+00  7.96083808e-01  1.00000000e+00\n",
      " -3.13810557e-01  1.00000000e+00  8.25335622e-01  1.00000000e+00\n",
      "  0.00000000e+00  6.05186403e-01  0.00000000e+00 -9.49485600e-01\n",
      "  0.00000000e+00  5.64642489e-01  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -3.11702639e-02 -1.00000001e-01\n",
      "  1.00665975e+00  9.99999702e-01  0.00000000e+00  7.96326727e-04\n",
      "  0.00000000e+00  2.08330005e-02 -2.08330005e-02  0.00000000e+00\n",
      "  0.00000000e+00 -1.21353102e+00  1.02764237e+00 -5.43710113e-01\n",
      " -6.20507956e-01 -1.68621123e-01 -8.90519083e-01 -2.27267575e+00\n",
      " -1.79743350e-01  1.09522790e-02  3.61086540e-02 -1.57110706e-01\n",
      "  8.49786580e-01  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  1.00000000e+00  6.82986602e-02  5.50731421e-02  1.55458272e-01\n",
      " -9.99999821e-01  4.35349299e-04  1.11648500e-04  4.73958120e-04\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  3.22426148e-02\n",
      " -1.02811627e-01  1.00510371e+00 -9.99999821e-01  4.35349299e-04\n",
      "  1.11648500e-04  4.73958120e-04  1.00000001e-01  1.80000007e-01\n",
      "  8.29999983e-01  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  1.00000000e+00  1.31900832e-01 -2.82111704e-01  1.74911052e-01\n",
      " -9.99999821e-01  4.35349299e-04  1.11648500e-04  4.73958120e-04\n",
      "  6.38913438e-02  3.37110698e-01 -1.97865982e-02  9.99996662e-01\n",
      "  7.94522285e-01  9.99999046e-01 -3.15292120e-01  9.99999940e-01\n",
      "  8.26570690e-01  9.99996960e-01 -2.58243480e-03  6.07234955e-01\n",
      " -1.36522448e-03 -9.48994637e-01 -3.35153309e-04  5.62832892e-01\n",
      " -2.45611998e-03 -8.81696343e-02  8.79413933e-02 -4.63254973e-02\n",
      " -5.32219447e-02 -1.12550864e-02 -7.50741586e-02 -8.39139596e-02\n",
      " -3.21072564e-02 -1.01830795e-01  1.00520778e+00  9.99999762e-01\n",
      " -4.35349357e-04 -1.11648507e-04  4.73958178e-04  1.35945436e-02\n",
      " -1.36644542e-02 -1.54534116e-01  1.52494073e-01  0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "print(inputs[0]) # 164 = 77 + 8 + 1 + 77 + 1 #STATE + ACTION + REWARD + NEXT STATE + TERMINAL, formatted in make_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 0\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "task_vector = indicators[idx, :].reshape(1, -1)\n",
    "\n",
    "labels = ['Object', 'Robot', 'Obstacle', 'Subtask']\n",
    "plt.figure(figsize=(10, 2))\n",
    "plt.imshow(task_vector, cmap=\"viridis\", aspect=\"auto\")\n",
    "plt.colorbar(label=\"Value\")\n",
    "plt.xticks(ticks=[2, 6, 10, 14], labels=labels, ha='right')\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.from_numpy(inputs).float()\n",
    "indicators = torch.from_numpy(indicators).float()\n",
    "dataset = torch.utils.data.TensorDataset(inputs, indicators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT DIM AT RESIDUALMLP SHOULD BE 164:  164\n",
      "Skipping normalization for dimensions [163].\n",
      "Means: tensor([ 6.3589e-02,  1.0651e-01,  8.6088e-01, -1.6371e-02, -1.2360e-02,\n",
      "         1.7438e-01,  9.5864e-01,  1.6769e-02,  9.9819e-03,  3.5345e-02,\n",
      "         5.4525e-01, -3.0514e-03,  9.8454e-02,  6.1551e-02,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00, -3.6970e-01,  1.0005e-01,  7.7840e-01,  4.5343e-01,\n",
      "         1.5858e-01,  8.5067e-02,  7.3912e-02,  1.0000e-01,  1.8000e-01,\n",
      "         8.3000e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00,\n",
      "         2.3125e-02, -4.1920e-02,  4.6128e-02,  4.5343e-01,  1.5858e-01,\n",
      "         8.5067e-02,  7.3912e-02,  3.6411e-02,  7.3486e-02, -3.0883e-02,\n",
      "         9.1670e-01,  5.0138e-01,  9.8886e-01,  3.1583e-04,  9.6788e-01,\n",
      "         9.6581e-01,  9.7018e-01,  2.6684e-01,  8.5701e-01,  9.7417e-02,\n",
      "        -9.9253e-01, -2.0960e-01,  2.3004e-01,  1.1947e-01,  2.5523e-02,\n",
      "         1.8551e-02,  2.0060e-03,  1.5352e-02, -1.0075e-02, -1.6298e-02,\n",
      "         1.4821e-02,  3.8410e-02,  1.1168e-01,  8.9260e-01,  9.6006e-01,\n",
      "         1.9058e-01,  1.4033e-01, -4.1342e-02,  7.8275e-03,  2.5215e-03,\n",
      "        -3.8880e-04,  7.2188e-04,  3.7693e-01,  7.4256e-01,  5.1869e-01,\n",
      "         1.2565e-01, -1.6088e-02, -3.2374e-01,  1.6236e-01,  6.0316e-01,\n",
      "         7.7182e-01,  6.3589e-02,  1.0651e-01,  8.6088e-01, -1.6372e-02,\n",
      "        -1.2361e-02,  1.7438e-01,  9.5864e-01,  1.6768e-02,  9.9818e-03,\n",
      "         3.5345e-02,  5.4525e-01, -3.0509e-03,  9.8454e-02,  6.1552e-02,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00, -3.6970e-01,  1.0005e-01,  7.7840e-01,\n",
      "         4.5343e-01,  1.5858e-01,  8.5067e-02,  7.3912e-02,  1.0000e-01,\n",
      "         1.8000e-01,  8.3000e-01,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         1.0000e+00,  2.3124e-02, -4.1920e-02,  4.6127e-02,  4.5343e-01,\n",
      "         1.5858e-01,  8.5067e-02,  7.3912e-02,  3.6411e-02,  7.3485e-02,\n",
      "        -3.0883e-02,  9.1670e-01,  5.0138e-01,  9.8886e-01,  3.1638e-04,\n",
      "         9.6788e-01,  9.6581e-01,  9.7018e-01,  2.6684e-01,  8.5701e-01,\n",
      "         9.7418e-02, -9.9253e-01, -2.0960e-01,  2.3004e-01,  1.1947e-01,\n",
      "         2.5523e-02,  1.8551e-02,  2.0060e-03,  1.5352e-02, -1.0076e-02,\n",
      "        -1.6298e-02,  1.4821e-02,  3.8411e-02,  1.1168e-01,  8.9260e-01,\n",
      "         9.6006e-01,  1.9058e-01,  1.4033e-01, -4.1342e-02,  7.8275e-03,\n",
      "         2.5215e-03, -3.8880e-04,  7.2186e-04,  0.0000e+00])\n",
      "Stds: tensor([8.1664e-02, 2.1506e-01, 3.5868e-02, 1.1163e-01, 1.0047e-01, 1.5009e-01,\n",
      "        7.1476e-02, 3.0459e-02, 2.7030e-02, 3.7160e-02, 8.0617e-01, 8.6865e-02,\n",
      "        1.4641e-01, 1.0167e-01, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.6115e-01, 2.7882e-01, 7.2388e-02,\n",
      "        8.4649e-01, 1.5094e-01, 1.2642e-01, 3.5676e-02, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 7.0501e-02,\n",
      "        2.2855e-01, 4.0252e-02, 8.4649e-01, 1.5094e-01, 1.2642e-01, 3.5676e-02,\n",
      "        8.1664e-02, 2.1506e-01, 3.5868e-02, 8.0588e-02, 9.7697e-02, 1.3336e-02,\n",
      "        1.2117e-01, 3.9928e-02, 3.7547e-02, 3.8463e-02, 2.8629e-01, 6.7929e-02,\n",
      "        1.1177e-01, 1.4517e-02, 1.3298e-01, 1.1354e-01, 2.0739e-01, 7.4812e-02,\n",
      "        6.6248e-02, 6.7737e-02, 6.4237e-02, 7.9397e-02, 7.3880e-02, 7.3296e-02,\n",
      "        7.7849e-02, 2.0597e-01, 4.8771e-02, 2.0349e-02, 1.0774e-01, 5.9359e-02,\n",
      "        7.0903e-02, 4.3121e-03, 7.4646e-03, 1.8168e-02, 1.8617e-02, 1.1633e+00,\n",
      "        1.1303e+00, 1.1526e+00, 1.2100e+00, 1.1800e+00, 1.1711e+00, 1.1349e+00,\n",
      "        9.9238e-01, 3.7866e-01, 8.1664e-02, 2.1506e-01, 3.5868e-02, 1.1163e-01,\n",
      "        1.0047e-01, 1.5009e-01, 7.1477e-02, 3.0459e-02, 2.7030e-02, 3.7160e-02,\n",
      "        8.0617e-01, 8.6866e-02, 1.4641e-01, 1.0167e-01, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.6115e-01,\n",
      "        2.7882e-01, 7.2388e-02, 8.4649e-01, 1.5094e-01, 1.2642e-01, 3.5676e-02,\n",
      "        1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05, 1.0000e-05,\n",
      "        1.0000e-05, 7.0501e-02, 2.2855e-01, 4.0252e-02, 8.4649e-01, 1.5094e-01,\n",
      "        1.2642e-01, 3.5676e-02, 8.1664e-02, 2.1506e-01, 3.5868e-02, 8.0588e-02,\n",
      "        9.7696e-02, 1.3336e-02, 1.2117e-01, 3.9929e-02, 3.7547e-02, 3.8463e-02,\n",
      "        2.8629e-01, 6.7928e-02, 1.1177e-01, 1.4517e-02, 1.3298e-01, 1.1354e-01,\n",
      "        2.0739e-01, 7.4812e-02, 6.6248e-02, 6.7737e-02, 6.4237e-02, 7.9397e-02,\n",
      "        7.3880e-02, 7.3296e-02, 7.7849e-02, 2.0597e-01, 4.8771e-02, 2.0349e-02,\n",
      "        1.0774e-01, 5.9359e-02, 7.0903e-02, 4.3121e-03, 7.4645e-03, 1.8168e-02,\n",
      "        1.8617e-02, 1.0000e+00])\n"
     ]
    }
   ],
   "source": [
    "diffusion = construct_diffusion_model(inputs=inputs, cond_dim=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mphamanhquanbill\u001b[0m (\u001b[33mcompositional-rl\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "/home/anhquanpham/.pyenv/versions/first_3.9.6/lib/python3.9/site-packages/wandb/sdk/lib/ipython.py:84: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import HTML, display  # type: ignore\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/anhquanpham/projects/compositional-rl-synth-data/notebooks/wandb/run-20250207_221614-sacnst3r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/compositional-rl/offline_rl_diffusion/runs/sacnst3r' target=\"_blank\">IIWA_Box_None_PickPlace</a></strong> to <a href='https://wandb.ai/compositional-rl/offline_rl_diffusion' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/compositional-rl/offline_rl_diffusion' target=\"_blank\">https://wandb.ai/compositional-rl/offline_rl_diffusion</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/compositional-rl/offline_rl_diffusion/runs/sacnst3r' target=\"_blank\">https://wandb.ai/compositional-rl/offline_rl_diffusion/runs/sacnst3r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/compositional-rl/offline_rl_diffusion/runs/sacnst3r?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7a0a4a22bb80>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb_project = 'offline_rl_diffusion'\n",
    "wandb_entity = ''\n",
    "wandb_group = 'diffusion_training'\n",
    "\n",
    "wandb.init(\n",
    "    project=wandb_project,\n",
    "    entity=wandb_entity,\n",
    "    group=wandb_group,\n",
    "    name=results_folder.split('/')[-1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 25911764.\n",
      "Using batch size: 1024\n",
      "Using cosine learning rate scheduler with warm restarts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COND VECTOR torch.Size([1024, 16])\n",
      "COND VECTOR 0 tensor([1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.])\n",
      "x shape after cat cond:  torch.Size([1024, 180])\n",
      "In Residual MLP:  torch.Size([1024, 164])\n",
      "OUTPUT DIM:  torch.Size([1024, 164])\n",
      "OUTPUT DIM AT MAIN NETWORK  torch.Size([1024, 164])\n",
      "In Residual MLP:  torch.Size([1024, 164])\n",
      "OUTPUT DIM:  torch.Size([1024, 164])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.0121:  10%|█         | 1/10 [00:02<00:21,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COND VECTOR torch.Size([1024, 16])\n",
      "COND VECTOR 0 tensor([1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.])\n",
      "x shape after cat cond:  torch.Size([1024, 180])\n",
      "In Residual MLP:  torch.Size([1024, 164])\n",
      "OUTPUT DIM:  torch.Size([1024, 164])\n",
      "OUTPUT DIM AT MAIN NETWORK  torch.Size([1024, 164])\n",
      "In Residual MLP:  torch.Size([1024, 164])\n",
      "OUTPUT DIM:  torch.Size([1024, 164])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.9951:  20%|██        | 2/10 [00:04<00:15,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COND VECTOR torch.Size([1024, 16])\n",
      "COND VECTOR 0 tensor([1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.])\n",
      "x shape after cat cond:  torch.Size([1024, 180])\n",
      "In Residual MLP:  torch.Size([1024, 164])\n",
      "OUTPUT DIM:  torch.Size([1024, 164])\n",
      "OUTPUT DIM AT MAIN NETWORK  torch.Size([1024, 164])\n",
      "In Residual MLP:  torch.Size([1024, 164])\n",
      "OUTPUT DIM:  torch.Size([1024, 164])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.9951:  20%|██        | 2/10 [00:04<00:19,  2.42s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(diffusion, dataset, results_folder\u001b[38;5;241m=\u001b[39mresults_folder)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/compositional-rl-synth-data/diffusion/elucidated_diffusion.py:415\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    413\u001b[0m data, cond \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), cond\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[0;32m--> 415\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_accumulate_every\n\u001b[1;32m    417\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.pyenv/versions/first_3.9.6/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/first_3.9.6/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/projects/compositional-rl-synth-data/diffusion/elucidated_diffusion.py:258\u001b[0m, in \u001b[0;36mElucidatedDiffusion.forward\u001b[0;34m(self, inputs, cond)\u001b[0m\n\u001b[1;32m    255\u001b[0m noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(inputs)\n\u001b[1;32m    256\u001b[0m noised_inputs \u001b[38;5;241m=\u001b[39m inputs \u001b[38;5;241m+\u001b[39m padded_sigmas \u001b[38;5;241m*\u001b[39m noise  \u001b[38;5;66;03m# alphas are 1. in the paper\u001b[39;00m\n\u001b[0;32m--> 258\u001b[0m denoised \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreconditioned_network_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoised_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigmas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m losses \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(denoised, inputs, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    260\u001b[0m losses \u001b[38;5;241m=\u001b[39m reduce(losses, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb ... -> b\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/projects/compositional-rl-synth-data/diffusion/elucidated_diffusion.py:116\u001b[0m, in \u001b[0;36mElucidatedDiffusion.preconditioned_network_forward\u001b[0;34m(self, noised_inputs, sigma, clamp, cond)\u001b[0m\n\u001b[1;32m    112\u001b[0m     sigma \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((batch,), sigma, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    114\u001b[0m padded_sigma \u001b[38;5;241m=\u001b[39m sigma\u001b[38;5;241m.\u001b[39mview(batch, \u001b[38;5;241m*\u001b[39m([\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent_shape)))\n\u001b[0;32m--> 116\u001b[0m net_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_in\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadded_sigma\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnoised_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_noise\u001b[49m\u001b[43m(\u001b[49m\u001b[43msigma\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_skip(padded_sigma) \u001b[38;5;241m*\u001b[39m noised_inputs \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_out(padded_sigma) \u001b[38;5;241m*\u001b[39m net_out\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clamp:\n",
      "File \u001b[0;32m~/.pyenv/versions/first_3.9.6/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/first_3.9.6/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/projects/compositional-rl-synth-data/diffusion/denoiser_network.py:283\u001b[0m, in \u001b[0;36mResidualMLPDenoiser.forward\u001b[0;34m(self, x, timesteps, cond)\u001b[0m\n\u001b[1;32m    281\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(x) \u001b[38;5;241m+\u001b[39m time_embed \u001b[38;5;66;03m#proj = 128, timeembeded = 128\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOUTPUT DIM AT MAIN NETWORK \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_mlp(x)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 283\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresidual_mlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/first_3.9.6/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/first_3.9.6/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/projects/compositional-rl-synth-data/diffusion/denoiser_network.py:216\u001b[0m, in \u001b[0;36mResidualMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn Residual MLP: \u001b[39m\u001b[38;5;124m\"\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOUTPUT DIM: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_linear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork(x)))\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_linear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n",
      "File \u001b[0;32m~/.pyenv/versions/first_3.9.6/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/first_3.9.6/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/first_3.9.6/lib/python3.9/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/first_3.9.6/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/first_3.9.6/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/projects/compositional-rl-synth-data/diffusion/denoiser_network.py:187\u001b[0m, in \u001b[0;36mResidualBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m#print(\"Shape x in Residual Block\", x)\u001b[39;00m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;66;03m#print(\"Fx shape \", (self.linear(self.activation(self.ln(x)))))\u001b[39;00m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;66;03m#print(\"Final shape\", (x + self.linear(self.activation(self.ln(x)))))\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/first_3.9.6/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/first_3.9.6/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/first_3.9.6/lib/python3.9/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer(diffusion, dataset, results_folder=results_folder)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gin.configurable\n",
    "class SimpleDiffusionGenerator:\n",
    "    def __init__(\n",
    "            self,\n",
    "            env: gym.Env,\n",
    "            ema_model,\n",
    "            num_sample_steps: int = 128,\n",
    "            sample_batch_size: int = 10, #fix to 100000 before pushing\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.diffusion = ema_model\n",
    "        self.diffusion.eval()\n",
    "        # Clamp samples if normalizer is MinMaxNormalizer\n",
    "        self.clamp_samples = isinstance(self.diffusion.normalizer, MinMaxNormalizer)\n",
    "        self.num_sample_steps = num_sample_steps\n",
    "        self.sample_batch_size = sample_batch_size\n",
    "        print(f'Sampling using: {self.num_sample_steps} steps, {self.sample_batch_size} batch size.')\n",
    "\n",
    "    def sample(\n",
    "            self,\n",
    "            num_samples: int,\n",
    "            cond: None,\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \n",
    "        assert num_samples % self.sample_batch_size == 0, 'num_samples must be a multiple of sample_batch_size'\n",
    "\n",
    "        if cond is not None:\n",
    "            cond = torch.from_numpy(cond).float().to(self.diffusion.device)\n",
    "            cond = cond.unsqueeze(0).expand(self.sample_batch_size, -1)\n",
    "\n",
    "        num_batches = num_samples // self.sample_batch_size\n",
    "        observations = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        next_observations = []\n",
    "        terminals = []\n",
    "\n",
    "        for i in range(num_batches):\n",
    "            print(f'Generating split {i + 1} of {num_batches}.')\n",
    "            sampled_outputs = self.diffusion.sample(\n",
    "                batch_size=self.sample_batch_size,\n",
    "                num_sample_steps=self.num_sample_steps,\n",
    "                clamp=self.clamp_samples,\n",
    "                cond=cond\n",
    "            )\n",
    "            sampled_outputs = sampled_outputs.cpu().numpy()\n",
    "\n",
    "            # Split samples into (s, a, r, s') format\n",
    "            transitions = split_diffusion_samples(sampled_outputs, self.env)\n",
    "            if len(transitions) == 4:\n",
    "                obs, act, rew, next_obs = transitions\n",
    "                terminal = np.zeros_like(next_obs[:, 0])\n",
    "            else:\n",
    "                obs, act, rew, next_obs, terminal = transitions\n",
    "            observations.append(obs)\n",
    "            actions.append(act)\n",
    "            rewards.append(rew)\n",
    "            next_observations.append(next_obs)\n",
    "            terminals.append(terminal)\n",
    "        observations = np.concatenate(observations, axis=0)\n",
    "        actions = np.concatenate(actions, axis=0)\n",
    "        rewards = np.concatenate(rewards, axis=0)\n",
    "        next_observations = np.concatenate(next_observations, axis=0)\n",
    "        terminals = np.concatenate(terminals, axis=0)\n",
    "\n",
    "        return observations, actions, rewards, next_observations, terminals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_indicator = get_task_indicator(robot, obj, obst, task)\n",
    "env = composuite.make(robot, obj, obst, task, use_task_id_obs=False, ignore_done=False)\n",
    "generator = SimpleDiffusionGenerator(env=env, ema_model=trainer.ema.ema_model)\n",
    "observations, actions, rewards, next_observations, terminals = generator.sample(num_samples=10, cond=task_indicator) #Fix numsamples to 100000 before pushing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\n",
    "    os.path.join(results_folder, 'samples.npz'),\n",
    "    observations=observations,\n",
    "    actions=actions,\n",
    "    rewards=rewards,\n",
    "    next_observations=next_observations,\n",
    "    terminals=terminals\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "First 3.9.6",
   "language": "python",
   "name": "first_3.9.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
